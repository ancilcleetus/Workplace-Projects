{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gadgeon Interview Preparation"
      ],
      "metadata": {
        "id": "F6dfM34sf_l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "D0XaQAZ-g549"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– TABLE OF CONTENTS\n",
        "\n",
        "- [Section 1]()\n",
        "  - [Subsection 1]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()\n",
        "  - [Subsection 2]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()\n",
        "- [Section 2]()\n",
        "  - [Subsection 1]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()\n",
        "  - [Subsection 2]()\n",
        "    - [Subsubsection 1]()\n",
        "    - [Subsubsection 2]()"
      ],
      "metadata": {
        "id": "SIhfEEeuxaVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wireless Power Transfer Circuit Schematic\n",
        "\n",
        "from IPython import display\n",
        "display.Image(\"data/images/Gadgeon-Interview-Preparation/Wireless-Power-Transfer-Circuit-Schematic.png\")"
      ],
      "metadata": {
        "id": "7UvkmeN48u0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "SrvW4d7PBo-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Machine Learning Metrics"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Precision and Recall"
      ],
      "metadata": {
        "id": "bO02IIpZOiqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definitions"
      ],
      "metadata": {
        "id": "Y_-cEblASyYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and Recall are two important metrics used to evaluate the performance of classification models, particularly in scenarios where the classes are imbalanced (e.g., predicting whether a patient has a disease based on ECG signals).\n",
        "\n",
        "- **Precision:** This metric indicates the accuracy of the positive predictions made by the model. It answers the question: \"Of all the instances that were predicted as positive, how many were actually positive?\"\n",
        "\n",
        "    $Precision = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} \\; + \\; \\text{False Positives (FP)}}$\n",
        "    â€‹\n",
        "- **Recall (also known as Sensitivity or True Positive Rate):** This metric measures the ability of the model to find all relevant cases (actual positives). It answers the question: \"Of all the actual positive instances, how many did we correctly predict as positive?\"\n",
        "\n",
        "    $Recall = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} \\; + \\; \\text{False Negatives (FN)}}$"
      ],
      "metadata": {
        "id": "xiTWlt-TTbbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "9z-zhouZS2Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a medical diagnosis scenario where we want to predict whether patients have a particular heart condition based on ECG data. Let's say we have the following results from our model:\n",
        "\n",
        "- **True Positives (TP):** The model correctly predicts that 30 patients have the condition.\n",
        "    \n",
        "- **False Positives (FP):** The model incorrectly predicts that 10 patients have the condition when they do not.\n",
        "\n",
        "- **False Negatives (FN):** The model fails to identify 5 patients who actually have the condition.\n",
        "\n",
        "Using these values, we can calculate Precision and Recall:\n",
        "\n",
        "- $Precision = \\frac {30}{30+10}=\\frac {30}{40} = 0.75$\n",
        "\n",
        "    This means that when the model predicts a patient has the condition, it is correct 75% of the time.\n",
        "\n",
        "- $Recall = \\frac {30}{30+5}=\\frac {30}{35} = 0.857$\n",
        "\n",
        "    This means that out of all patients who actually have the condition, the model correctly identifies approximately 85.7%."
      ],
      "metadata": {
        "id": "Ynw9sklGTf5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance of Precision and Recall"
      ],
      "metadata": {
        "id": "yrx8yIJ2S5Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Precision is crucial in scenarios where false positives can lead to unnecessary treatments or anxiety for patients. For example, if a model falsely identifies healthy patients as having a heart condition, it may lead to unnecessary medical interventions.\n",
        "\n",
        "- Recall is important when missing a positive case could have serious consequences. In our example, failing to identify a patient with a heart condition could lead to severe health risks."
      ],
      "metadata": {
        "id": "yLBfpRQZTjen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Trade-off between Precision and Recall"
      ],
      "metadata": {
        "id": "QIsgQ4tcS5Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of COVID-19 classification, Precision and Recall are critical metrics that help evaluate the effectiveness of diagnostic models.\n",
        "\n",
        "- **Precision** measures the accuracy of positive predictions. In COVID-19 detection, it answers the question: \"Of all patients predicted to have COVID-19, how many actually have it?\" A high precision indicates that when the model predicts a patient has COVID-19, it is likely correct.\n",
        "\n",
        "- **Recall**, on the other hand, measures the ability of the model to identify all actual positive cases. It answers: \"Of all patients who actually have COVID-19, how many did we correctly identify?\" High recall means that most patients with COVID-19 are correctly diagnosed."
      ],
      "metadata": {
        "id": "Rk3SRbyVVV1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Trade-off Explained"
      ],
      "metadata": {
        "id": "HDQUjMFNTEZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trade-off between precision and recall occurs because increasing one often leads to a decrease in the other. This is particularly relevant in high-stakes situations like COVID-19 diagnosis:\n",
        "\n",
        "1. **High Recall, Lower Precision:** If a model is tuned to maximize recall, it will classify more patients as positive for COVID-19 to ensure that most actual cases are detected. However, this can lead to a higher number of false positives (healthy patients incorrectly diagnosed as having COVID-19). For instance, if a model identifies 95% of true COVID-19 cases (high recall) but also falsely identifies many healthy patients as positive (lower precision), it may create unnecessary alarm and resource allocation for those false positives.\n",
        "\n",
        "2. **High Precision, Lower Recall:** Conversely, if the model is adjusted to maximize precision, it will be more conservative in its positive predictions. This means fewer healthy patients will be misclassified as having COVID-19 (higher precision), but some actual cases may be missed (lower recall). In this scenario, a patient with COVID-19 might be incorrectly classified as negative, potentially leading to further spread of the virus.\n",
        "\n",
        "Given the contagious nature of COVID-19:\n",
        "\n",
        "- **High Recall is Critical:** It is often more important to ensure that all infected individuals are identified to prevent further transmission. Missing an actual case (false negative) can lead to severe public health consequences.\n",
        "\n",
        "- **Acceptable Precision Levels:** While high precision is desirable to avoid unnecessary panic and treatment for healthy individuals, in urgent public health scenarios like a pandemic, slightly lower precision may be acceptable if it means capturing more true cases."
      ],
      "metadata": {
        "id": "GQq78_B0V3eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Facial Recognition System: High Precision Low Recall Application Scenario"
      ],
      "metadata": {
        "id": "A3EZzCBNT76x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scenario**\n",
        "\n",
        "In a facial recognition system designed to authenticate users for secure access (e.g., smartphones, secure buildings), the primary goal is to ensure that only authorized individuals can gain entry. Here, it is crucial to minimize the chances of unauthorized access, even if it means that some authorized users may be incorrectly denied access.\n",
        "\n",
        "**High Precision**\n",
        "\n",
        "- **Definition:** High precision in this context means that when the system identifies a person as authorized (positive prediction), it is very likely correct. For instance, if the system claims that 90% of the individuals it recognizes are indeed authorized users, this indicates high precision.\n",
        "    \n",
        "- **Implication:** This high precision reduces the risk of unauthorized individuals gaining access. If the model predicts that a person is authorized, there is a 90% chance that they actually are. This reliability is critical in security applications where false positives (incorrectly granting access) can lead to serious security breaches.\n",
        "\n",
        "**Low Recall**\n",
        "\n",
        "- **Definition:** Low recall means that while the system is very accurate in its positive predictions, it fails to recognize a significant number of actual authorized users. For example, if out of 100 authorized users, the system only recognizes 30 correctly (true positives), while failing to recognize 70 (false negatives), the recall would be low.\n",
        "    \n",
        "- **Implication:** This low recall indicates that many legitimate users are being denied access because their faces are not recognized by the system. While this might be acceptable in high-security scenarios where preventing unauthorized access is paramount, it can frustrate users who experience repeated denials.\n",
        "\n",
        "**Trade-off Justification**\n",
        "\n",
        "In this application:\n",
        "\n",
        "- **The cost of false positives (granting access to unauthorized individuals) is much higher than the cost of false negatives (denying access to authorized users). Therefore, designers prioritize precision over recall.**\n",
        "\n",
        "- Users may prefer a system that is very accurate when it does grant access, even if it occasionally denies legitimate users. In such cases, having a reliable verification process with high precision ensures that security remains intact.\n"
      ],
      "metadata": {
        "id": "_AoP4_CqXqwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tumor Detection in Medical Imaging: Low Precision High Recall Application Scenario"
      ],
      "metadata": {
        "id": "C7ItApV_UHQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scenario**\n",
        "\n",
        "In the medical field, especially in oncology, early detection of tumors can significantly improve treatment outcomes. A machine learning model is developed to analyze medical images (such as MRI or CT scans) to identify potential tumors.\n",
        "\n",
        "**High Recall**\n",
        "\n",
        "- **Definition:** High recall in this context means that the model is very effective at identifying actual tumor cases. For instance, if out of 100 patients with tumors, the model correctly identifies 90 of them as having tumors, this indicates high recall.\n",
        "    \n",
        "- **Implication:** This high recall is crucial because missing a tumor (false negative) could lead to delayed treatment and worsen the patient's prognosis. In this scenario, it is vital to catch as many true cases as possible.\n",
        "\n",
        "**Low Precision**\n",
        "\n",
        "- **Definition:** Low precision means that while the model identifies most actual tumors, it also incorrectly labels many healthy cases as positive (false positives). For example, if the model predicts that 120 patients have tumors (including both true and false positives), but only 30 of those predictions are correct, the precision would be low.\n",
        "    \n",
        "- **Implication:** This results in a situation where many healthy patients are subjected to unnecessary anxiety and additional testing due to false positives. While it is critical to catch all possible tumor cases, the downside is that a significant number of healthy individuals are misclassified.\n",
        "\n",
        "**Trade-off Justification**\n",
        "\n",
        "In this application:\n",
        "\n",
        "- **The cost of false negatives (failing to identify an actual tumor) is much higher than that of false positives (incorrectly identifying a healthy patient as having a tumor). Therefore, the model is designed with a focus on maximizing recall.**\n",
        "    \n",
        "- Medical professionals often prefer a system that ensures they do not miss any potential cancer cases, even if it means dealing with a higher number of false alarms. This approach allows for further investigation and testing for those flagged as positive."
      ],
      "metadata": {
        "id": "IMu1-IY6Yw85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ROC and AUC"
      ],
      "metadata": {
        "id": "ZjVrdxwQZlEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definitions"
      ],
      "metadata": {
        "id": "MZZ9kxlscMI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Receiver Operating Characteristic (ROC) Curve:** The ROC curve is a graphical representation used to evaluate the performance of a binary classification model at various threshold settings. It plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)**.\n",
        "\n",
        "- **True Positive Rate (TPR)**, also known as sensitivity or recall, is calculated as:\n",
        "\n",
        "    $TPR = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} \\; + \\; \\text{False Negatives (FN)}}$\n",
        "\n",
        "- **False Positive Rate (FPR)** is calculated as:\n",
        "\n",
        "    $FPR = \\frac{\\text{False Positives (FP)}}{\\text{False Positives (FP)} \\; + \\; \\text{True Negatives (TN)}}$\n",
        "\n",
        "**Area Under the Curve (AUC):** The AUC quantifies the overall performance of the model by measuring the area under the ROC curve. The value of AUC ranges from 0 to 1:\n",
        "\n",
        "- An AUC of 1 indicates a perfect model that can perfectly distinguish between positive and negative classes.\n",
        "\n",
        "- An AUC of 0.5 suggests that the model performs no better than random chance.\n",
        "\n",
        "- An AUC less than 0.5 indicates that the model is performing worse than random guessing."
      ],
      "metadata": {
        "id": "-S4AKNFPcX9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How ROC and AUC Work"
      ],
      "metadata": {
        "id": "CGvc7inKdd4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Generating the ROC Curve:**\n",
        "\n",
        "- To create an ROC curve, you calculate TPR and FPR for different threshold values ranging from 0 to 1.\n",
        "    \n",
        "- As you adjust the threshold, you can observe how TPR and FPR change, allowing you to plot these values on a graph.\n",
        "\n",
        "2. **Interpreting the ROC Curve:**\n",
        "\n",
        "- The curve starts at the point (0,0) and ends at (1,1).\n",
        "    \n",
        "- A curve that bows towards the top left corner indicates a better-performing model, while a curve closer to the diagonal line (from (0,0) to (1,1)) indicates poor performance."
      ],
      "metadata": {
        "id": "dvp_HvmudsvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "yqae1D0febva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a binary classification model designed to detect whether patients have a specific disease based on test results. Here's how you might visualize its performance using an ROC curve:\n",
        "\n",
        "- **Model Predictions:** After running your model, you get predicted probabilities for each patient indicating their likelihood of having the disease.\n",
        "\n",
        "- **Thresholds:** You evaluate thresholds from 0.0 to 1.0 in increments (e.g., 0.1).\n",
        "\n",
        "For example:\n",
        "\n",
        "- At a threshold of **0.3**, suppose your model predicts:\n",
        "\n",
        "    - TP = 80\n",
        "    - FP = 10\n",
        "    - FN = 20\n",
        "    - TN = 90\n",
        "\n",
        "Calculating TPR and FPR:\n",
        "\n",
        "- TPR = $\\frac {80}{80+20}$ = 0.8\n",
        "- FPR = $\\frac {10}{10+90}$ = 0.1\n",
        "\n",
        "You would plot this point on your ROC curve at coordinates (0.1, 0.8). As you continue adjusting thresholds and calculating TPR and FPR, you generate more points until you can connect them to form your ROC curve."
      ],
      "metadata": {
        "id": "uCVj-Fykeg7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance of AUC"
      ],
      "metadata": {
        "id": "H6iBvnK_fLAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AUC provides a single scalar value that summarizes the model's ability to discriminate between classes across all thresholds:\n",
        "\n",
        "- A higher AUC value indicates better performance in distinguishing between positive and negative classes.\n",
        "    \n",
        "- For instance, if your model has an AUC of 0.85, it means there is an 85% chance that it will rank a randomly chosen positive instance higher than a randomly chosen negative instance."
      ],
      "metadata": {
        "id": "GvRm3buKfYQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differences in AUC-ROC for Multi-Class Classification"
      ],
      "metadata": {
        "id": "n5DdzJzqf1NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Binary vs. Multi-Class:**\n",
        "\n",
        "- In binary classification, the ROC curve is straightforward, plotting True Positive Rate (TPR) against False Positive Rate (FPR) for one positive class versus one negative class.\n",
        "    \n",
        "- In multi-class classification, where there are three or more classes, the ROC curve needs to be adapted because each class can be considered as a positive class while treating all other classes as negative.\n",
        "\n",
        "2. **One-vs-Rest (OvR) Approach:**\n",
        "\n",
        "- The most common method for creating ROC curves in multi-class settings is the **One-vs-Rest (OvR)** approach. For each class:\n",
        "\n",
        "    - Treat that class as the positive class and all other classes as negative.    \n",
        "    - Generate a separate ROC curve for each class.\n",
        "    \n",
        "- For example, if you have three classes (A, B, C), you would create:\n",
        "        \n",
        "    - One ROC curve for class A vs. classes B and C.\n",
        "    - One ROC curve for class B vs. classes A and C.\n",
        "    - One ROC curve for class C vs. classes A and B.\n",
        "\n",
        "3. **One-vs-One (OvO) Approach:**\n",
        "\n",
        "- Another method is the **One-vs-One (OvO)** approach, which involves creating a ROC curve for every pair of classes.\n",
        "    \n",
        "- For three classes (A, B, C), you would create:\n",
        "        \n",
        "    - One ROC curve for class A vs. class B.\n",
        "    - One ROC curve for class A vs. class C.\n",
        "    - One ROC curve for class B vs. class C.\n",
        "    \n",
        "- This method can become computationally intensive as the number of classes increases.\n",
        "\n",
        "4. **Micro and Macro Averaging:**\n",
        "\n",
        "- After generating multiple ROC curves using either OvR or OvO methods, you can summarize the performance using **micro** and **macro averaging:**\n",
        "        \n",
        "    - **Micro Averaging:** Computes global TPR and FPR by aggregating contributions from all classes before calculating the AUC. This approach treats all instances equally and is useful when you want to emphasize the performance across all samples.\n",
        "        \n",
        "    - **Macro Averaging:** Calculates the AUC for each class separately and then takes the average of these values. This method treats all classes equally regardless of their size, which can be beneficial when dealing with imbalanced datasets."
      ],
      "metadata": {
        "id": "GkNDqFzKgkgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of AUC-ROC for Multi-Class Classification"
      ],
      "metadata": {
        "id": "tz2F8ATshiMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we have a dataset containing images of cats, dogs, and rabbits. Our goal is to train a classifier that can accurately identify these animals based on their features.\n",
        "\n",
        "- **Step 1: Training the Model**\n",
        "\n",
        "    We train a multi-class classifier (e.g., a neural network or logistic regression) on this dataset. After training, the model outputs predicted probabilities for each class for every instance in the test set.\n",
        "\n",
        "- **Step 2: Applying ROC and AUC**\n",
        "\n",
        "    In the One-vs-Rest (OvR) approach, we treat each class as a positive class while combining all other classes as negative. This means we will create three separate ROC curves:\n",
        "\n",
        "    1. **ROC Curve for Cats:**\n",
        "        - Treat \"Cats\" as the positive class.\n",
        "        - Combine \"Dogs\" and \"Rabbits\" as the negative class.\n",
        "        - Calculate True Positive Rate (TPR) and False Positive Rate (FPR) at various thresholds.\n",
        "    2. **ROC Curve for Dogs:**\n",
        "        - Treat \"Dogs\" as the positive class.\n",
        "        - Combine \"Cats\" and \"Rabbits\" as the negative class.\n",
        "        - Calculate TPR and FPR similarly.\n",
        "    3. **ROC Curve for Rabbits:**\n",
        "        - Treat \"Rabbits\" as the positive class.\n",
        "        - Combine \"Cats\" and \"Dogs\" as the negative class.\n",
        "        - Calculate TPR and FPR accordingly.\n",
        "\n",
        "    - **Example Calculation for OvR**\n",
        "\n",
        "        Assume we calculate TPR and FPR at different thresholds for each class. Here's an example of what the results might look like:\n",
        "\n",
        "        - **Cats:**\n",
        "            - TPR = 0.85, FPR = 0.10\n",
        "        - **Dogs:**\n",
        "            - TPR = 0.80, FPR = 0.15\n",
        "        - **Rabbits:**\n",
        "            - TPR = 0.90, FPR = 0.05\n",
        "\n",
        "        Using these values, we can plot three ROC curves on the same graph.\n",
        "    \n",
        "    - **Area Under the Curve (AUC)**\n",
        "    \n",
        "        After plotting the ROC curves, we calculate the AUC for each curve:\n",
        "\n",
        "        - AUC for Cats: **0.88**\n",
        "        - AUC for Dogs: **0.82**\n",
        "        - AUC for Rabbits: **0.91**\n",
        "\n",
        "- **Step 3: Averaging AUC Scores**\n",
        "\n",
        "To summarize model performance across all classes, we can use two averaging methods:\n",
        "\n",
        "1. **Macro Averaging:**\n",
        "        \n",
        "    - This method calculates the average of AUCs across all classes without considering class imbalance.\n",
        "\n",
        "    $\\text{Macro AUC} = \\frac {\\text{AUC}_\\text{Cats} + \\text{AUC}_\\text{Dogs} + \\text{AUC}_\\text{Rabbits}}{3} = \\frac {0.88+0.82+0.91}{3} \\approx 0.87$\n",
        "    \n",
        "2. **Micro Averaging:**\n",
        "        \n",
        "    - Micro averaging aggregates contributions from all classes before calculating metrics, treating each instance equally.\n",
        "    \n",
        "    - For micro averaging in multi-class settings, you typically sum all true positives, false positives, etc., across classes before calculating TPR and FPR."
      ],
      "metadata": {
        "id": "XIy0bcb3inMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Pitfalls When Interpreting the AUC-ROC Curve"
      ],
      "metadata": {
        "id": "ZYLCbPEto4tP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the AUC-ROC curve can be insightful, but there are several common pitfalls that can lead to misleading conclusions. Here are some key pitfalls to be aware of:\n",
        "\n",
        "1. **Imbalanced Datasets:**\n",
        "\n",
        "    - **Issue:** The ROC curve can provide an overly optimistic assessment of model performance when dealing with imbalanced datasets. In such cases, the False Positive Rate (FPR) may appear very low due to a large number of True Negatives (TN), making the model seem more effective than it actually is.\n",
        "    \n",
        "    - **Example:** In a dataset with 95% negative cases and 5% positive cases, a model that predicts all instances as negative could still achieve a high AUC simply because it has many TNs and few FPs, despite failing to identify any positive cases.\n",
        "\n",
        "2. **Ignoring Cost of Errors:**\n",
        "\n",
        "    - **Issue:** AUC-ROC does not take into account the different costs associated with false positives and false negatives. In many applications, especially in healthcare, the consequences of misclassifications can vary significantly.\n",
        "    \n",
        "    - **Example:** In a medical diagnosis scenario, failing to identify a disease (false negative) might have severe consequences compared to falsely diagnosing it (false positive). Relying solely on AUC could lead to poor decision-making if the costs of errors are not considered.\n",
        "\n",
        "3. **Threshold Independence:**\n",
        "\n",
        "    - **Issue:** AUC measures performance across all possible thresholds, which may include thresholds that are not practically relevant for specific applications. This can obscure meaningful insights about model performance at clinically relevant thresholds.\n",
        "    \n",
        "    - **Example:** A model might have a high AUC but perform poorly at the threshold that is most relevant for clinical decisions. This means that while the overall performance looks good, it may not translate to effective real-world use.\n",
        "\n",
        "4. **Misleading Comparisons:**\n",
        "\n",
        "    - **Issue:** When comparing models based on AUC values, caution is needed, especially if ROC curves intersect. Simply comparing AUC values may not provide a complete picture of model performance.\n",
        "    \n",
        "    - **Example:** If two models have similar AUC values but one model performs significantly better at clinically important thresholds while the other does not, relying solely on AUC could lead to choosing the less effective model.\n",
        "\n",
        "5. **Interpretation of AUC Values:**\n",
        "\n",
        "    - **Issue:** Misinterpretation of what different AUC values imply can lead to incorrect conclusions about model effectiveness. An AUC close to 0.5 indicates random guessing, while an AUC above 0.7 is often considered acceptable.\n",
        "    \n",
        "    - **Example:** Clinicians might assume that an AUC of 0.8 indicates excellent performance without considering how it translates into actual clinical outcomes or whether it consistently performs well across relevant thresholds.\n",
        "\n",
        "6. **Overfitting and Model Complexity:**\n",
        "\n",
        "    - **Issue:** High AUC scores can sometimes be achieved by overly complex models that do not generalize well to unseen data. This can lead to overfitting where the model performs well on training data but poorly on test data.\n",
        "    \n",
        "    - **Example:** A complex model might achieve an AUC of 0.95 on training data but drop significantly when evaluated on validation data due to its inability to generalize.\n",
        "\n",
        "Here are some alternative metrics that can complement the AUC-ROC curve for a more comprehensive evaluation of model performance:\n",
        "\n",
        "1. **Precision and Recall:**\n",
        "\n",
        "    - **Precision:** Measures the accuracy of positive predictions. It answers the question: \"Of all instances predicted as positive, how many were actually positive?\"\n",
        "    \n",
        "    - **Recall:** Measures the ability to identify all actual positive instances. It answers: \"Of all actual positives, how many did we correctly identify?\"\n",
        "    \n",
        "    - **Use Case:** These metrics are particularly important in medical diagnosis, where failing to identify a disease (false negative) can have serious consequences.\n",
        "\n",
        "2. **Area Under the Precision-Recall Curve (AUC-PR):**\n",
        "\n",
        "    - **Definition:** This metric summarizes the trade-off between precision and recall across different thresholds.\n",
        "    \n",
        "    - **Use Case:** Particularly useful for imbalanced datasets where the positive class is rare. It focuses on the performance of the classifier with respect to the positive (minority) class.\n",
        "    \n",
        "    - **Strengths:** Unlike AUC-ROC, which can be overly optimistic in imbalanced settings, AUC-PR provides a clearer picture of how well the model performs on the minority class.\n",
        "\n",
        "3. **Logarithmic Loss (Log Loss):**\n",
        "\n",
        "    - **Definition:** Log loss measures the performance of a classification model where predictions are probabilities between 0 and 1. It penalizes incorrect classifications with a heavier weight for confident wrong predictions.\n",
        "    \n",
        "    - **Use Case:** Useful when you want to evaluate how well your predicted probabilities align with actual outcomes.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "While the AUC-ROC curve is a valuable tool for assessing model performance, it is essential to be aware of these common pitfalls when interpreting its results. Understanding these limitations helps ensure that decisions based on ROC analysis are informed and appropriate for the specific context in which a model is being applied. To mitigate these issues, consider using additional metrics (like precision and recall) and conducting thorough evaluations at clinically relevant thresholds alongside ROC analysis."
      ],
      "metadata": {
        "id": "lzYpzDd5pwzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does the Choice of Threshold affect the AUC-ROC Curve?"
      ],
      "metadata": {
        "id": "QqFxGAdmsOkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of threshold in a classification model significantly affects the AUC-ROC curve and the interpretation of model performance. Here's a detailed explanation of how this relationship works:"
      ],
      "metadata": {
        "id": "cbTY_nYJsZyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding Thresholds in ROC Analysis"
      ],
      "metadata": {
        "id": "gNKuP_8MscLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Threshold Definition:**\n",
        "\n",
        "    - A threshold is a probability value that determines how a predicted score is classified into binary outcomes (positive or negative). For instance, if a model predicts a probability of 0.7 for a positive class, and the threshold is set at 0.5, the prediction will be classified as positive.\n",
        "\n",
        "2. **Impact on True Positive Rate (TPR) and False Positive Rate (FPR):**\n",
        "\n",
        "    - As you adjust the threshold:\n",
        "        \n",
        "        - **Lowering the Threshold:** Increases TPR (sensitivity) because more instances are classified as positive. However, this also increases FPR, leading to more false positives.\n",
        "        \n",
        "        - **Raising the Threshold:** Decreases TPR because fewer instances are classified as positive, but also decreases FPR, resulting in fewer false positives.\n",
        "\n",
        "    This inverse relationship means that as you increase sensitivity (TPR), you simultaneously increase the rate of false positives (FPR) and vice versa."
      ],
      "metadata": {
        "id": "8qYxG7BMIQlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How Threshold Choice Affects AUC-ROC"
      ],
      "metadata": {
        "id": "n5XAhKOksgzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **AUC-ROC Overview:**\n",
        "\n",
        "    - The AUC measures the area under the ROC curve, which plots TPR against FPR across various thresholds. A higher AUC indicates better model performance in distinguishing between positive and negative classes.\n",
        "\n",
        "2. **Threshold Independence of AUC:**\n",
        "\n",
        "    - One of the key properties of AUC is that it is threshold-invariant; it summarizes model performance across all possible thresholds rather than being tied to a specific one. This means that while individual thresholds affect TPR and FPR, the overall AUC remains a holistic measure of performance.\n",
        "\n",
        "3. **Choosing Optimal Thresholds:**\n",
        "\n",
        "    - Although AUC provides a general assessment, selecting an optimal threshold for practical applications depends on the specific costs associated with false positives and false negatives:\n",
        "        \n",
        "        - If false negatives are costly (e.g., missing a cancer diagnosis), you might choose a lower threshold to maximize TPR, even if it leads to more false positives.\n",
        "        \n",
        "        - Conversely, if false positives are costly (e.g., unnecessary treatments), you might opt for a higher threshold to minimize FPR, accepting a lower TPR."
      ],
      "metadata": {
        "id": "voqiFTpFIlrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example Scenario"
      ],
      "metadata": {
        "id": "ZPijTyZ2sg89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a medical diagnostic test for detecting a disease:\n",
        "\n",
        "- **Threshold Set at 0.3:**\n",
        "    - TPR = 0.85 (high sensitivity)\n",
        "    - FPR = 0.15\n",
        "    - This setting captures most actual cases of the disease but may lead to many healthy individuals being incorrectly diagnosed.\n",
        "\n",
        "- **Threshold Set at 0.7:**\n",
        "    - TPR = 0.60 (lower sensitivity)\n",
        "    - FPR = 0.05\n",
        "    - This setting reduces false positives significantly but misses some true cases of the disease."
      ],
      "metadata": {
        "id": "nfXnb-8wJoi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary"
      ],
      "metadata": {
        "id": "IvPgyCfrshG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The choice of threshold directly influences TPR and FPR, shaping the ROC curve's appearance.\n",
        "    \n",
        "- While AUC provides an overall measure of model performance across all thresholds, selecting an appropriate threshold for specific applications requires careful consideration of the implications of false positives and negatives.\n",
        "    \n",
        "- Ultimately, understanding how threshold adjustments affect both individual metrics and overall AUC helps practitioners make informed decisions about model deployment in real-world scenarios.\n",
        "\n",
        "By considering these factors, you can better interpret ROC curves and select thresholds that align with your specific goals and constraints in classification tasks."
      ],
      "metadata": {
        "id": "tkSMr44OKd0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Micro and Macro Averaging to Find Different Metrics"
      ],
      "metadata": {
        "id": "9s0xzTWuK2zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Micro and macro averaging are techniques used to evaluate the performance of classification models, particularly in multi-class classification problems. They provide different perspectives on model performance by handling class contributions differently. Hereâ€™s a detailed explanation of both methods, along with examples to illustrate their differences."
      ],
      "metadata": {
        "id": "bT86e8TmNBAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definitions"
      ],
      "metadata": {
        "id": "JmUf_JwsNC1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Micro Averaging:**\n",
        "\n",
        "    - Micro averaging aggregates the contributions of all classes to compute the overall performance metrics. It treats each instance equally, regardless of its class label.\n",
        "    \n",
        "    - In micro averaging, you sum up the true positives (TP), false positives (FP), and false negatives (FN) across all classes before calculating precision, recall, or F1 score.\n",
        "\n",
        "- **Macro Averaging:**\n",
        "\n",
        "    - Macro averaging calculates the performance metrics for each class independently and then takes the average. It gives equal weight to each class, regardless of how many instances belong to each class.\n",
        "    \n",
        "    - In macro averaging, you compute precision, recall, or F1 score for each class separately and then average these values."
      ],
      "metadata": {
        "id": "h6ilkQGdNvRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Use Each Method"
      ],
      "metadata": {
        "id": "Goxu9BCcNG-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Micro Averaging:** Use this when you want to give equal weight to each instance. This is particularly useful in imbalanced datasets where you want the overall performance to reflect the model's ability to classify instances correctly.\n",
        "\n",
        "- **Macro Averaging:** Use this when you want to treat all classes equally, regardless of their size. This is useful when you want to assess how well your model performs across all classes without being biased by the majority class."
      ],
      "metadata": {
        "id": "fYAfzaCiOHoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Scenario"
      ],
      "metadata": {
        "id": "lNQQFUIfNNDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a multi-class classification problem with three classes: **Cats**, **Dogs**, and **Rabbits**. Let's say we have the following confusion matrix based on model predictions:\n",
        "\n",
        "| Actual \\ Predicted | Cats | Dogs | Rabbits |\n",
        "| :----------------- | :--- | :--- | :------ |\n",
        "| Cats | 30 | 5 | 2 |\n",
        "| Dogs | 3 | 25 | 1 |\n",
        "| Rabbits | 4 | 2 | 27 |\n",
        "\n",
        "From this confusion matrix, we can derive the following:\n",
        "\n",
        "- **True Positives (TP):**\n",
        "    \n",
        "    - Cats: 30\n",
        "    - Dogs: 25\n",
        "    - Rabbits: 27\n",
        "\n",
        "- **False Positives (FP):**\n",
        "\n",
        "    - FP for Cats $\\implies$ Predicted as Cats but Actually Dogs or Rabbits = 3 + 4 = 7\n",
        "    - FP for Dogs $\\implies$ Predicted as Dogs but Actually Cats or Rabbits = 5 + 2 = 7\n",
        "    - FP for Rabbits $\\implies$ Predicted as Rabbits but Actually Cats or Dogs = 2 + 1 = 3\n",
        "\n",
        "- **False Negatives (FN):**\n",
        "\n",
        "    - FN for Cats $\\implies$ Instances that are actually Cats but were predicted as Dogs or Rabbits = 5 + 2 = 7\n",
        "    - FN for Dogs $\\implies$ Instances that are actually Dogs but were predicted as Cats or Rabbits = 3 + 1 = 4\n",
        "    - FN for Rabbits $\\implies$ Instances that are actually Rabbits but were predicted as Cats or Dogs = 4 + 2 = 6"
      ],
      "metadata": {
        "id": "TuAPcFSoQpuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Micro Averaging Calculation"
      ],
      "metadata": {
        "id": "yhdKyCVeNOAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate micro averages, we sum up all TP, FP, and FN:\n",
        "\n",
        "- Total TP = 30 + 25 + 27=82\n",
        "- Total FP = 7 + 7 + 3 = 17\n",
        "- Total FN = 7 + 4 + 6 = 17\n",
        "\n",
        "Now we can calculate micro precision and recall:\n",
        "\n",
        "- **Micro Precision:**\n",
        "    \n",
        "    $\\text{Micro Precision} = \\frac {\\text{Total TP}}{\\text{Total TP} + \\text{Total FP}} = \\frac {82}{82 + 17} = \\frac {82}{99} = 0.828$\n",
        "\n",
        "- **Micro Recall:**\n",
        "    \n",
        "    $\\text{Micro Recall} = \\frac {\\text{Total TP}}{\\text{Total TP} + \\text{Total FN}} = \\frac {82}{82 + 17} = \\frac {82}{99} = 0.828$"
      ],
      "metadata": {
        "id": "g2IEw5IXajtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Macro Averaging Calculation"
      ],
      "metadata": {
        "id": "wu0wFWXONPOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For macro averages, we calculate precision and recall for each class independently:\n",
        "\n",
        "1. **Precision for Each Class:**\n",
        "    \n",
        "    - **Cats:**\n",
        "    \n",
        "    $\\text{P}_\\text{Cats} = \\frac {\\text{TP}_\\text{Cats}}{\\text{TP}_\\text{Cats} + \\text{FP}_\\text{Cats}} = \\frac {30}{30 + 7} = \\frac {30}{37} = 0.811$\n",
        "\n",
        "    - **Dogs:**\n",
        "    \n",
        "    $\\text{P}_\\text{Dogs} = \\frac {\\text{TP}_\\text{Dogs}}{\\text{TP}_\\text{Dogs} + \\text{FP}_\\text{Dogs}} = \\frac {25}{25 + 7} = \\frac {25}{32} = 0.781$\n",
        "\n",
        "    - **Rabbits:**\n",
        "    \n",
        "    $\\text{P}_\\text{Rabbits} = \\frac {\\text{TP}_\\text{Rabbits}}{\\text{TP}_\\text{Rabbits} + \\text{FP}_\\text{Rabbits}} = \\frac {27}{27 + 3} = \\frac {27}{30} = 0.9$\n",
        "\n",
        "2. **Recall for Each Class:**\n",
        "    \n",
        "    - **Cats:**\n",
        "    \n",
        "    $\\text{R}_\\text{Cats} = \\frac {\\text{TP}_\\text{Cats}}{\\text{TP}_\\text{Cats} + \\text{FN}_\\text{Cats}} = \\frac {30}{30 + 7} = \\frac {30}{37} = 0.811$\n",
        "\n",
        "    - **Dogs:**\n",
        "    \n",
        "    $\\text{R}_\\text{Dogs} = \\frac {\\text{TP}_\\text{Dogs}}{\\text{TP}_\\text{Dogs} + \\text{FN}_\\text{Dogs}} = \\frac {25}{25 + 4} = \\frac {25}{29} = 0.862$\n",
        "\n",
        "    - **Rabbits:**\n",
        "    \n",
        "    $\\text{R}_\\text{Rabbits} = \\frac {\\text{TP}_\\text{Rabbits}}{\\text{TP}_\\text{Rabbits} + \\text{FN}_\\text{Rabbits}} = \\frac {27}{27 + 6} = \\frac {27}{33} = 0.818$\n",
        "\n",
        "3. **Calculating Macro Averages:**\n",
        "\n",
        "    - **Macro Precision:**\n",
        "\n",
        "    $\\text{Macro Precision} = \\frac {\\text{P}_\\text{Cats} + \\text{P}_\\text{Dogs} + \\text{P}_\\text{Rabbits}}{3} = \\frac {0.811 + 0.781 + 0.9}{3} = \\frac {2.492}{3} = 0.8307$\n",
        "\n",
        "    - **Macro Recall:**\n",
        "\n",
        "    $\\text{Macro Recall} = \\frac {\\text{R}_\\text{Cats} + \\text{R}_\\text{Dogs} + \\text{R}_\\text{Rabbits}}{3} = \\frac {0.811 + 0.862 + 0.818}{3} = \\frac {2.491}{3} = 0.8303$"
      ],
      "metadata": {
        "id": "KmxI8BJRbuEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Results"
      ],
      "metadata": {
        "id": "jX4lVSRINPOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric | Micro Average | Macro Average |\n",
        "| :----- | :------------ | :------------ |\n",
        "| Precision | 0.828 | 0.8307 |\n",
        "| Recall | 0.828 | 0.8303 |"
      ],
      "metadata": {
        "id": "q18jXossf1JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "f6_HJ4qpNPOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Micro and macro averaging provide valuable insights into model performance in multi-class classification tasks:\n",
        "\n",
        "- **Micro Averaging** gives equal weight to each instance, making it sensitive to class imbalances and reflecting overall performance across all predictions.\n",
        "\n",
        "- **Macro Averaging** treats all classes equally regardless of their size, providing insights into how well the model performs across all classes without being biased by larger classes.\n",
        "\n",
        "Choosing between micro and macro averaging depends on the specific context of your application and whether you want to prioritize overall accuracy or equal treatment of all classes in your evaluation metrics."
      ],
      "metadata": {
        "id": "Wnlmgi_fgVxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. F1 Score"
      ],
      "metadata": {
        "id": "3g5ZH0NWgn6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition"
      ],
      "metadata": {
        "id": "0rlUN8nCiGNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F1 Score** is a metric used to evaluate the performance of a classification model, particularly in situations where the classes are imbalanced. It is the harmonic mean of **Precision** and **Recall**, providing a single score that balances both metrics. The F1 Score is especially useful when you want to find an optimal balance between precision and recall.\n",
        "\n",
        "- Formula $\\implies$ $\\text{F1 Score} = 2 \\times \\frac {\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$"
      ],
      "metadata": {
        "id": "C4Hr4wgYilfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance of F1 Score"
      ],
      "metadata": {
        "id": "LkHuPhl6iRlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Balancing Act:** The F1 Score provides a way to combine precision and recall into a single metric, making it easier to understand model performance, especially when dealing with imbalanced datasets.\n",
        "    \n",
        "- **Focus on Positive Class:** In many applications, such as fraud detection or medical diagnosis, identifying the positive class correctly is more important than simply achieving high accuracy. The F1 Score emphasizes this aspect."
      ],
      "metadata": {
        "id": "2zhA92DdjSY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Scenario"
      ],
      "metadata": {
        "id": "SgIFlXEsiRDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a binary classification problem where we want to predict whether an email is spam or not. After running our model, we obtain the following confusion matrix:\n",
        "\n",
        "| Actual \\ Predicted | Spam (Positive) | Not Spam (Negative) |\n",
        "| :----------------- | :-------------- | :------------------ |\n",
        "| Spam | 40 | 10 |\n",
        "| Not Spam | 5 | 45 |\n",
        "\n",
        "From this confusion matrix, we can derive:\n",
        "\n",
        "- **True Positives (TP):** 40 (correctly predicted spam emails)\n",
        "    \n",
        "- **False Positives (FP):** 5 (not spam emails incorrectly predicted as spam)\n",
        "    \n",
        "- **False Negatives (FN):** 10 (spam emails incorrectly predicted as not spam)"
      ],
      "metadata": {
        "id": "OU9wJd8zjhT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-by-Step Calculation"
      ],
      "metadata": {
        "id": "221bKhQxiSiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Calculate Precision:**\n",
        "\n",
        "    $\\text{Precision} = \\frac {\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac {40}{40 + 5} = \\frac {40}{45} = 0.889$\n",
        "\n",
        "2. **Calculate Recall:**\n",
        "\n",
        "    $\\text{Recall} = \\frac {\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac {40}{40 + 10} = \\frac {40}{50} = 0.8$\n",
        "\n",
        "3. **Calculate F1 Score:**\n",
        "\n",
        "    $\\text{F1 Score} = 2 \\times \\frac {\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac {0.889 \\times 0.8}{0.889 + 0.8} \\approx 0.842$"
      ],
      "metadata": {
        "id": "KRPRA7nakRAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "hHjiTJ30iSib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the F1 Score of approximately **0.842** indicates a good balance between precision and recall for our spam detection model. The F1 Score is particularly valuable in scenarios where false negatives carry significant consequences, such as in medical diagnoses or fraud detection, allowing practitioners to make informed decisions about model performance based on a single metric that reflects both precision and recall. By using the F1 Score alongside other metrics like accuracy and AUC-ROC, you can gain a comprehensive understanding of your model's performance and its suitability for your specific application needs."
      ],
      "metadata": {
        "id": "AZzXHlhul1Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Other Evaluation Metrics for Multi-Class Classification"
      ],
      "metadata": {
        "id": "sx7GhK4Gl_ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to commonly used metrics like precision, recall, and F1 score, there are several other evaluation metrics that are important for assessing the performance of multi-class classification models. These metrics provide insights into different aspects of model performance, especially when dealing with imbalanced datasets or specific application requirements.\n",
        "\n",
        "Here are some key evaluation metrics for multi-class classification:"
      ],
      "metadata": {
        "id": "YzpdrXPdn1EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Accuracy"
      ],
      "metadata": {
        "id": "Xzdwv_ign4wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Definition:** Accuracy measures the proportion of correctly predicted instances out of the total instances. It is calculated as:\n",
        "\n",
        "    - $\\text{Accuracy} = \\frac {\\text{Total Correct Predictions}}{\\text{Total Predictions}} = \\frac {\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
        "\n",
        "- **Example:** If a model correctly classifies 80 out of 100 instances in a three-class problem (Cats, Dogs, Rabbits), the accuracy would be:\n",
        "\n",
        "    - $\\text{Accuracy} = \\frac {80}{100} = 0.80 or 80 \\%$\n",
        "\n",
        "- **Limitations:** While accuracy is a straightforward metric, it can be misleading in cases of class imbalance. For example, if 95 out of 100 instances belong to one class, a model that predicts all instances as that class could achieve high accuracy without actually being effective at distinguishing between classes."
      ],
      "metadata": {
        "id": "iB4nkn-7n_Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Log Loss (Cross-Entropy Loss)"
      ],
      "metadata": {
        "id": "sHYK_aauo6Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Definition:** Log loss measures the performance of a classification model whose output is a probability value between 0 and 1. It quantifies the difference between the predicted probabilities and the actual class labels. The formula for log loss is:\n",
        "\n",
        "    - $\\text{Log Loss} = - \\frac {1}{N} \\Sigma_{i = 1}^{N} (y_i \\log p_i + (1 - y_i) \\log (1 - p_i))$ where\n",
        "        - $y_i$ is the actual label (0 or 1).\n",
        "        - $p_i$ is the predicted probability of the positive class.\n",
        "\n",
        "- **Example:** If a model predicts probabilities for three classes and the actual labels are known, log loss can be calculated to assess how well the predicted probabilities align with the actual outcomes.\n",
        "\n",
        "- **Limitations:** Log loss can be sensitive to outliers and may not provide a clear picture of performance if used alone."
      ],
      "metadata": {
        "id": "z-GHJnnApIMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Macro-Averaged Metrics"
      ],
      "metadata": {
        "id": "ydaDZ6JWo8af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Macro-Averaged Precision/Recall/F1 Score:** These metrics calculate precision, recall, or F1 score for each class independently and then average these scores without considering class imbalance. This approach treats all classes equally.\n",
        "\n",
        "    $\\text{Macro Precision} = \\frac {\\Sigma_{i = 1}^C \\text{Precision}_i}{C}$ where $C$ is the number of classes.\n"
      ],
      "metadata": {
        "id": "RKEDNk9AqNjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Weighted-Averaged Metrics"
      ],
      "metadata": {
        "id": "SKVY5F54o8_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Weighted-Averaged Precision/Recall/F1 Score:** These metrics compute precision, recall, or F1 score for each class and then average them while weighting by the number of true instances for each class (support). This approach accounts for class imbalance.\n",
        "\n",
        "    $\\text{Weighted Precision} = \\frac {\\Sigma_{i = 1}^C \\text{Precision}_i \\times \\text{Support}_i{\\Sigma_{i = 1}^C \\text{Support}_i}$ where $\\text{Support}_i$ is the number of actual instances for each class.\n"
      ],
      "metadata": {
        "id": "pc-Mk1zuq0Nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "ioJzjO8CIC3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Deep Learning Model Architectures"
      ],
      "metadata": {
        "id": "-SCKaEhyIC3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "kDHJkWMeIIC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Deep Learning on Time Series Data"
      ],
      "metadata": {
        "id": "2-EV8A-kIIC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "rD5eOg_6IOAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Practical Applications and Datasets"
      ],
      "metadata": {
        "id": "jprmrdS3IOAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "3YWh3kAFISO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Additional Topics"
      ],
      "metadata": {
        "id": "mzj5EoKFISO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "U897-NGnYhHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difficulty: ${\\color{green}{Easy}}$\n",
        "Difficulty: ${\\color{orange}{Medium}}$\n",
        "Difficulty: ${\\color{red}{Hard}}$"
      ],
      "metadata": {
        "id": "Pf0qrlJcsUj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning as subset of ML\n",
        "\n",
        "from IPython import display\n",
        "display.Image(\"data/images/ML.jpg\")"
      ],
      "metadata": {
        "id": "juNAxVBFg7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}